{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRGydOoZWJpe"
      },
      "source": [
        "# Inisialisasi Kaggle API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLxdnOjDyNuw"
      },
      "outputs": [],
      "source": [
        "! chmod 600 /content/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaDcX-DNWovc",
        "outputId": "f83751d1-f522-4766-b9eb-2db4bbfed5a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle\n",
            "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxqv41ZaW-23",
        "outputId": "28b740bf-58b6-455e-eef8-fda86676a7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "koi-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d nafianmuh/koi-dataset #kaggle datasets download -d nafianmuh/koi-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGIhxwQzNoWg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me-RmRnhXDiF"
      },
      "outputs": [],
      "source": [
        "zip_file = zipfile.ZipFile('koi-dataset.zip','r')\n",
        "zip_file.extractall('/tmp/dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMe8JxRNDwtb",
        "outputId": "ec9ba64a-9dfa-4115-83b7-5a997c67e12e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['pyright-1667-Inpnc6KTkCWQ', 'dataset', 'dap_multiplexer.f2910a898ae4.root.log.INFO.20220526-073056.43', 'pyright-243-ve52WkkX1OhS', 'dap_multiplexer.INFO', 'initgoogle_syslog_dir.0', '__pycache__', 'pyright-1709-ajBsGjJlBso5', 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', 'python-languageserver-cancellation', 'debugger_1scv9wc31g', '__autograph_generated_fileorxp0t5u.py', '__autograph_generated_file2b30w_l8.py', 'koi', 'pyright-1667-H3X7Yli7TAvT', 'pyright-202-TbK6XWQRTOJE', 'pyright-202-gZXtopJ7nFa1', 'pyright-243-JiHvB7NkJIeN', 'pyright-1709-lytShWU1R4ei']\n",
            "['Hikarimono', 'Koromo', 'Asagi', 'Utsurimono', 'Sanke', 'Shusui', 'Goshiki', 'Bekko', 'Showa', 'Kohaku']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir('/tmp/'))\n",
        "print(os.listdir('/tmp/dataset/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHQ6OVqoYqXz"
      },
      "outputs": [],
      "source": [
        "#class_names = ['Utsurimono', 'Sanke', 'Showa','Chagoi', 'Shusui','Shiro', 'Asagi', 'Koromo', 'Tancho', 'Kohaku']\n",
        "class_names = ['Hikarimono', 'Koromo', 'Asagi', 'Utsurimono', 'Sanke', 'Shusui', 'Goshiki', 'Bekko', 'Showa', 'Kohaku']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcstuAG-LjXx",
        "outputId": "96003d8f-0370-4740-e75e-f6071472139d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 7 images of Hikarimono koi.\n",
            "There are 39 images of Koromo koi.\n",
            "There are 33 images of Asagi koi.\n",
            "There are 32 images of Utsurimono koi.\n",
            "There are 65 images of Sanke koi.\n",
            "There are 44 images of Shusui koi.\n",
            "There are 85 images of Goshiki koi.\n",
            "There are 67 images of Bekko koi.\n",
            "There are 23 images of Showa koi.\n",
            "There are 35 images of Kohaku koi.\n"
          ]
        }
      ],
      "source": [
        "source_path = '/tmp/dataset/'\n",
        "source_path_koi = []\n",
        "for names in class_names: \n",
        "  source_path_koi.append(os.path.join(source_path, names))\n",
        "\n",
        "for labels in source_path_koi:\n",
        "  print(f\"There are {len(os.listdir(labels))} images of {os.path.basename(labels)} koi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGIIykS1XlUn",
        "outputId": "08132126-431e-41df-cc26-121d31aa967d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/dataset/Hikarimono\n",
            "/tmp/dataset/Koromo\n",
            "/tmp/dataset/Asagi\n",
            "/tmp/dataset/Utsurimono\n",
            "/tmp/dataset/Sanke\n",
            "/tmp/dataset/Shusui\n",
            "/tmp/dataset/Goshiki\n",
            "/tmp/dataset/Bekko\n",
            "/tmp/dataset/Showa\n",
            "/tmp/dataset/Kohaku\n"
          ]
        }
      ],
      "source": [
        "for labels in source_path_koi:\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co-fgVPCMmlV"
      },
      "outputs": [],
      "source": [
        "# Define root directory\n",
        "root_dir = '/tmp/koi'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "# GRADED FUNCTION: create_train_test_dirs\n",
        "def create_train_test_dirs(root_path):\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # HINT:\n",
        "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
        "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
        "  training_dir = 'training'\n",
        "  testing_dir = 'testing'\n",
        "  training_path = os.path.join(root_path,training_dir)\n",
        "  testing_path = os.path.join(root_path, testing_dir)\n",
        "\n",
        "  for path in source_path_koi:\n",
        "    os.makedirs(os.path.join(training_path, os.path.basename(path)))\n",
        "\n",
        "    os.makedirs(os.path.join(testing_path, os.path.basename(path)))\n",
        "\n",
        "  pass\n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_test_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQPmpsrtNhpQ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: split_data\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "\n",
        "  ### START CODE HERE\n",
        "  content = random.sample(os.listdir(SOURCE),len(os.listdir(SOURCE)))\n",
        "\n",
        "  for index, name in enumerate(content):\n",
        "    if os.path.getsize(os.path.join(SOURCE, name)) == 0:\n",
        "      print('{} is zero length, so ignoring.'.format(name))\n",
        "      del content[index]\n",
        "      \n",
        "  \n",
        "  splitsize = int(len(content)) * SPLIT_SIZE\n",
        "  count = 0 \n",
        "  \n",
        "  for x in content:\n",
        "    if count != int(splitsize):\n",
        "      copyfile(os.path.join(SOURCE, x), os.path.join(TRAINING,x))\n",
        "      count += 1\n",
        "    else:\n",
        "      copyfile(os.path.join(SOURCE, x), os.path.join(TESTING,x))\n",
        "  \n",
        "  ### END CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt3dqakKPPto",
        "outputId": "aab2fb4a-76f3-4f5a-e5f9-59160b72b9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/koi/testing\n",
            "/tmp/koi/training\n",
            "/tmp/koi/testing/Hikarimono\n",
            "/tmp/koi/testing/Koromo\n",
            "/tmp/koi/testing/Asagi\n",
            "/tmp/koi/testing/Utsurimono\n",
            "/tmp/koi/testing/Sanke\n",
            "/tmp/koi/testing/Shusui\n",
            "/tmp/koi/testing/Goshiki\n",
            "/tmp/koi/testing/Bekko\n",
            "/tmp/koi/testing/Showa\n",
            "/tmp/koi/testing/Kohaku\n",
            "/tmp/koi/training/Hikarimono\n",
            "/tmp/koi/training/Koromo\n",
            "/tmp/koi/training/Asagi\n",
            "/tmp/koi/training/Utsurimono\n",
            "/tmp/koi/training/Sanke\n",
            "/tmp/koi/training/Shusui\n",
            "/tmp/koi/training/Goshiki\n",
            "/tmp/koi/training/Bekko\n",
            "/tmp/koi/training/Showa\n",
            "/tmp/koi/training/Kohaku\n"
          ]
        }
      ],
      "source": [
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X73uQTvuUtdT",
        "outputId": "c4c62c16-48ce-443e-a0a2-fcbf7007cf53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "salman/tmp/dataset/\n"
          ]
        }
      ],
      "source": [
        "print('/tmp/dataset'.join(['salman', '/']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWsh9IrnOSiI"
      },
      "source": [
        "['Utsurimono', 'Goshiki', 'Sanke', 'Showa', 'Shusui', 'Asagi', 'Koromo', 'Bekko', 'Kohaku', 'Hikarimono']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHzX-2npNsTA",
        "outputId": "111fccf9-4a41-4eb2-9dbe-d76c9bf25184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "There are 5 images of Hikarimono for training\n",
            "\n",
            "There are 31 images of Koromo for training\n",
            "\n",
            "There are 26 images of Asagi for training\n",
            "\n",
            "There are 25 images of Utsurimono for training\n",
            "\n",
            "There are 52 images of Sanke for training\n",
            "\n",
            "There are 35 images of Shusui for training\n",
            "\n",
            "There are 68 images of Goshiki for training\n",
            "\n",
            "There are 53 images of Bekko for training\n",
            "\n",
            "There are 18 images of Showa for training\n",
            "\n",
            "There are 28 images of Kohaku for training\n",
            "\n",
            "There are 2 images of Hikarimono for testing\n",
            "\n",
            "There are 8 images of Koromo for testing\n",
            "\n",
            "There are 7 images of Asagi for testing\n",
            "\n",
            "There are 7 images of Utsurimono for testing\n",
            "\n",
            "There are 13 images of Sanke for testing\n",
            "\n",
            "There are 9 images of Shusui for testing\n",
            "\n",
            "There are 17 images of Goshiki for testing\n",
            "\n",
            "There are 14 images of Bekko for testing\n",
            "\n",
            "There are 5 images of Showa for testing\n",
            "\n",
            "There are 7 images of Kohaku for testing\n"
          ]
        }
      ],
      "source": [
        "# Test your split_data function\n",
        "\n",
        "# Define paths\n",
        "SOURCE_DIR_KOI = []\n",
        "for koi in class_names:\n",
        "  SOURCE_DIR_KOI.append('/tmp/dataset/' + ''.join([koi, '/']))\n",
        "#UTSURIMONO_SOURCE_DIR = \"/tmp/dataset/Utsurimono/\"\n",
        "#GOSHIKI_SOURCE_DIR = \"/tmp/dataset/Goshiki/\"\n",
        "#SANKE_SOURCE_DIR = \"/tmp/dataset/Sanke/\"\n",
        "#SHOWA_SOURCE_DIR = \"/tmp/dataset/Showa/\"\n",
        "#SHUSUI_SOURCE_DIR = \"/tmp/dataset/Shusui/\"\n",
        "#ASAGI_SOURCE_DIR = \"/tmp/dataset/Asagi/\"\n",
        "#KOROMO_SOURCE_DIR = \"/tmp/dataset/Koromo/\"\n",
        "#BEKKO_SOURCE_DIR = \"/tmp/dataset/Bekko/\"\n",
        "#KOHAKU_SOURCE_DIR = \"/tmp/dataset/Kohaku/\"\n",
        "#HIKARIMONO_SOURCE_DIR = \"/tmp/dataset/Hikarimono/\"\n",
        "\n",
        "\n",
        "\n",
        "TRAINING_DIR = \"/tmp/koi/training/\"\n",
        "TESTING_DIR = \"/tmp/koi/testing/\"\n",
        "\n",
        "TRAINING_KOI_DIR = []\n",
        "TESTING_KOI_DIR = []\n",
        "\n",
        "for koi in class_names:\n",
        "  TRAINING_KOI_DIR.append(os.path.join(TRAINING_DIR, ''.join([koi, '/'])))\n",
        "  TESTING_KOI_DIR.append(os.path.join(TESTING_DIR, ''.join([koi, '/'])))\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Utsurimono/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Utsurimono/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Goshiki/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Goshiki/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Sanke/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Sanke/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Showa/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Showa/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Shusui/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Shusui/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Asagi/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Asagi/\")\n",
        "\n",
        "#TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"Koromo/\")\n",
        "#TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"Koromo/\")\n",
        "\n",
        "#TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"Bekko/\")\n",
        "#TESTING_DOGS_DIR = os.path.join(TESTING_DIR, \"Bekko/\")\n",
        "\n",
        "#TRAINING_KOHAKU_DIR = os.path.join(TRAINING_DIR, \"Kohaku/\")\n",
        "#TESTING_DOGS_DIR = os.path.join(TESTING_DIR, \"Kohaku/\")\n",
        "\n",
        "#TRAINING_HIKARIMONO_DIR = os.path.join(TRAINING_DIR, \"Hikarimono/\")\n",
        "#TESTING_HIKARIMONO_DIR = os.path.join(TESTING_DIR, \"Hikarimono/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "for koi in TRAINING_KOI_DIR:\n",
        "  if len(os.listdir(koi)) > 0:\n",
        "    for file in os.scandir(koi):\n",
        "      os.remove(file.path)\n",
        "\n",
        "for koi in TESTING_KOI_DIR:\n",
        "  if len(os.listdir(koi)) > 0:\n",
        "    for file in os.scandir(koi):\n",
        "      os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .8\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "for index in range(len(SOURCE_DIR_KOI)):\n",
        "  split_data(SOURCE_DIR_KOI[index], TRAINING_KOI_DIR[index], TESTING_KOI_DIR[index],split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "for training in TRAINING_KOI_DIR:\n",
        "  print(f\"\\nThere are {len(os.listdir(training))} images of {os.path.basename(os.path.dirname(training))} for training\")\n",
        "\n",
        "for testing in TESTING_KOI_DIR:\n",
        "  print(f\"\\nThere are {len(os.listdir(testing))} images of {os.path.basename(os.path.dirname(testing))} for testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPzi9u-LYK_d"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the arguments to augment the images)\n",
        "  train_datagen = ImageDataGenerator(rescale= 1/255.)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory= TRAINING_DIR,\n",
        "                                                      batch_size= 32,\n",
        "                                                      class_mode= 'categorical',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  validation_datagen = ImageDataGenerator(rescale= 1/255.)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory= VALIDATION_DIR,\n",
        "                                                                batch_size= 32,\n",
        "                                                                class_mode= 'categorical',\n",
        "                                                                target_size=(150, 150))\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXozTi8DYZWl"
      },
      "outputs": [],
      "source": [
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0EOPKaVaGa9"
      },
      "source": [
        "# Scratch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_NPvpbAYgAm"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      #first convolution\n",
        "      tf.keras.layers.Conv2D(16, (3,3), activation='relu',input_shape=(150,150,3)),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      #second convolution\n",
        "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2), \n",
        "      #tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "      #tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      tf.keras.layers.Flatten(), \n",
        "      # 512 neuron hidden layer\n",
        "      tf.keras.layers.Dense(512, activation='relu'), \n",
        "      # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  ])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer = 'adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AsOmRN1YrZU"
      },
      "outputs": [],
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kkZYawpLXic"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xXhI2Y457yU"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMeGeV9Z3TQm"
      },
      "outputs": [],
      "source": [
        "# Download the inception v3 weights\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMQLasRf6eY8"
      },
      "outputs": [],
      "source": [
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wFMldiI6hE3"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_pre_trained_model\n",
        "def create_pre_trained_model(local_weights_file):\n",
        "  ### START CODE HERE\n",
        "  pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "                                  include_top = False, \n",
        "                                  weights = None) \n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  # Make all the layers in the pre-trained model non-trainable\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return pre_trained_model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6RNpfMR6kmG"
      },
      "outputs": [],
      "source": [
        "pre_trained_model = create_pre_trained_model(local_weights_file)\n",
        "\n",
        "# Print the model summary\n",
        "pre_trained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zhnrhlz66qg1"
      },
      "outputs": [],
      "source": [
        "total_params = pre_trained_model.count_params()\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in pre_trained_model.trainable_weights])\n",
        "\n",
        "print(f\"There are {total_params:,} total parameters in this model.\")\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOBh7gI16utX"
      },
      "outputs": [],
      "source": [
        "# Define a Callback class that stops training once accuracy reaches 99.9%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.999):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3EJ62tL64WG"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: output_of_last_layer\n",
        "def output_of_last_layer(pre_trained_model):\n",
        "  ### START CODE HERE\n",
        "  last_desired_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_desired_layer.output_shape)\n",
        "  last_output = last_desired_layer.output\n",
        "  print('last layer output: ', last_output)\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return last_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn2y8Nf4657u"
      },
      "outputs": [],
      "source": [
        "last_output = output_of_last_layer(pre_trained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYYU9-n07Byc"
      },
      "outputs": [],
      "source": [
        "# Print the type of the pre-trained model\n",
        "print(f\"The pretrained model has type: {type(pre_trained_model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yK4AuEd7MFm"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_final_model\n",
        "def create_final_model(pre_trained_model, last_output):\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Add a fully connected layer with 1024 hidden units and ReLU activation\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  # Create the complete model by using the Model class\n",
        "  model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer = 'adam', \n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  ### END CODE HERE\n",
        "  \n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Eo-1VWx79FO"
      },
      "outputs": [],
      "source": [
        "# Save your model in a variable\n",
        "model = create_final_model(pre_trained_model, last_output)\n",
        "\n",
        "# Inspect parameters\n",
        "total_params = model.count_params()\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n",
        "\n",
        "print(f\"There are {total_params:,} total parameters in this model.\")\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvWerTo88A8R"
      },
      "outputs": [],
      "source": [
        "# Run this and see how many epochs it should take before the callback\n",
        "# fires, and stops training at 99.9% accuracy\n",
        "# (It should take a few epochs)\n",
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 100,\n",
        "                    verbose = 2,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQCg0Ts9uMMr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x /= 255.\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "\n",
        "  class_names = ['Asagi', 'Bekko', 'Goshiki', 'Hikarimono', 'Kohaku', 'Koromo', 'Sanke', 'Showa', 'Shusui', 'Utsurimono']\n",
        "  \n",
        "  max = 0\n",
        "  index = 0\n",
        "  \n",
        "  for num in classes[0]:\n",
        "    if num >= max:\n",
        "      max = num\n",
        "    #index+=1\n",
        "\n",
        "  print(class_names[index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5Otwguz-p7V"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKaXuvJnEcVJ"
      },
      "source": [
        "# Export Model to TFLite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8ErEHyzzBGCm"
      },
      "outputs": [],
      "source": [
        "KOI_SAVED_MODEL = \"exp_saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xypbt7t2Hm8g",
        "outputId": "be3555f0-e8df-4250-a225-c32d6990f695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: exp_saved_model/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(model, KOI_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ox0ZyTiHr3k",
        "outputId": "378f3c31-73ae-48dc-81cd-873260493c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['input_1'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 150, 150, 3)\n",
            "      name: serving_default_input_1:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['dense_3'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ],
      "source": [
        "%%bash -s $KOI_SAVED_MODEL\n",
        "saved_model_cli show --dir $1 --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "SEj8dorCH0NV"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(KOI_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03n7kO-LH80C",
        "outputId": "6209f54d-bfdf-4256-e728-187494284321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['serving_default']\n",
            "((), {'input_1': TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_1')})\n",
            "{'dense_3': TensorSpec(shape=(None, 10), dtype=tf.float32, name='dense_3')}\n"
          ]
        }
      ],
      "source": [
        "print(list(loaded.signatures.keys()))\n",
        "infer = loaded.signatures[\"serving_default\"]\n",
        "print(infer.structured_input_signature)\n",
        "print(infer.structured_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tde9JIDsYCPu"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(KOI_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "PkipIp91YSxh"
      },
      "outputs": [],
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "7lvEMqmUYj_z"
      },
      "outputs": [],
      "source": [
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlnXyi9JYXLl",
        "outputId": "2e0289e8-6882-41ff-e843-d930bbda355f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "tflite_model = converter.convert()\n",
        "tflite_model_file = 'converted_model.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Koi Tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
